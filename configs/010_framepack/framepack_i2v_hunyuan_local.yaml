flow:
  target: videotuna.flow.framepack.HunyuanVideoPackedFlow
  params:
    task: "i2v-14B"                   # The task to run (choices from WAN_CONFIGS.keys())
    # ckpt_path: results/train/train_framepack_i2v_hunyuan_lora_20250528134626/checkpoints                    # The path to the checkpoint directory.
    ckpt_path: ./hf_download    
    offload_model: true               # Whether to offload the model to CPU after each model forward.
    ulysses_size: 1                   # The size of the ulysses parallelism in DiT.
    ring_size: 1                      # The size of the ring attention parallelism in DiT.
    t5_fsdp: false                    # Whether to use FSDP for T5.
    t5_cpu: false                     # Whether to place T5 model on CPU.
    dit_fsdp: false                   # Whether to use FSDP for DiT.
    use_prompt_extend: false          # Whether to use prompt extend.
    prompt_extend_method: "local_qwen" # The prompt extend method to use (choices: dashscope, local_qwen)
    prompt_extend_model: null         # The prompt extend model to use.
    prompt_extend_target_lang: "zh"   # The target language of prompt extend (choices: zh, en)
    seed: 42                     # The seed to use for generating the image or video

    scheduler_config: __is_first_stage__

    high_vram: True
    
    denoiser_config:
      target: videotuna.models.framepack.models.hunyuan_video_packed.HunyuanVideoTransformer3DModelPacked
      use_from_pretrained: false
      params:
        # pretrained_model_name_or_path: lllyasviel/FramePackI2V_HY
        # ckpt_path: ${flow.params.ckpt_path}/denoiser-047-000001950.ckpt
        attention_head_dim: 128
        guidance_embeds: true
        has_clean_x_embedder: true
        has_image_proj: true
        image_proj_dim: 1152
        in_channels: 16
        mlp_ratio: 4.0
        num_attention_heads: 24
        num_layers: 20
        num_refiner_layers: 2
        num_single_layers: 40
        out_channels: 16
        patch_size: 2
        patch_size_t: 1
        pooled_projection_dim: 768
        qk_norm: "rms_norm"
        rope_axes_dim: [16, 56, 56]
        rope_theta: 256.0
        text_embed_dim: 4096

    first_stage_config:
      target: diffusers.AutoencoderKLHunyuanVideo
      params:
        pretrained_model_name_or_path: hunyuanvideo-community/HunyuanVideo
        subfolder: "vae"
        load_dtype: fp16 # bf16 5b / fp16 2B 
        # torch_dtype: float16 # bf16 5b / fp16 2B 


    cond_stage_config:
      target: transformers.LlamaModel
      params:
        pretrained_model_name_or_path: hunyuanvideo-community/HunyuanVideo
        subfolder: "text_encoder"
        torch_dtype: float16 # bf16 5b / fp16 2B 
        # device: cuda

    tokenizer_config: 
      target: transformers.LlamaTokenizerFast
      params:
        pretrained_model_name_or_path: hunyuanvideo-community/HunyuanVideo
        subfolder: "tokenizer"
        torch_dtype: float16 # bf16 5b / fp16 2B 


    cond_stage_2_config:
      target: transformers.CLIPTextModel
      params:
        pretrained_model_name_or_path: hunyuanvideo-community/HunyuanVideo
        subfolder: "text_encoder_2"
        torch_dtype: float16 # bf16 5b / fp16 2B 
    
    tokenizer_config_2: 
      target: transformers.CLIPTokenizer
      params:
        pretrained_model_name_or_path: hunyuanvideo-community/HunyuanVideo
        subfolder: "tokenizer_2"
    
    feature_extractor_config:
      target: transformers.SiglipImageProcessor
      params:
        pretrained_model_name_or_path: lllyasviel/flux_redux_bfl
        subfolder: "feature_extractor"
    
    image_encoder_config:
      target: transformers.SiglipVisionModel
      params:
        pretrained_model_name_or_path: lllyasviel/flux_redux_bfl
        subfolder: "image_encoder"
        torch_dtype: float16
    
    lora_config: 
      target: peft.LoraConfig
      params:
        r: 4
        lora_alpha: 1.0 
        init_lora_weights: True
        target_modules: ["to_k", "to_q", "to_v", "to_out.0"]

inference:
  video_length_in_second: 5
  latent_window_size: 9
  steps: 25
  cfg: 1.0
  gs: 10.0
  rs: 0.0
  gpu_memory_preservation: 6
  use_teacache: True
  mp4_crf: 16
  prompt: "a cute cartoon dinosaur is running, cartoon style"
  input_image_path: "test-i2v-480/1.png"
  n_prompt: ""
  seed: 31337

  mode: i2v
  ckpt_path: results/train/train_framepack_i2v_hunyuan_lora_20250528134626/checkpoints/only_trained_model
  # trained_ckpt: denoiser-025-000001050.ckpt
  # ckpt_path: results/train/train_framepack_i2v_hunyuan_lora_20250511235315/checkpoints/only_trained_model
  trained_ckpt: denoiser-001-000000050.ckpt
  savedir: results/i2v/framepack
  height: 240
  width: 416
  prompt_dir: "inputs/i2v/576x1024"
  solver: "unipc"           
  num_inference_steps: 40                
  time_shift: 3.0                
  unconditional_guidance_scale: 5.0                       
  frames: 81
  n_samples_prompt: 1
  bs: 1
  savefps: 16
  enable_model_cpu_offload: true

  # mapping:
  #   inference.ckpt_path : flow.params.ckpt_path
  #   inference.seed : flow.params.seed
  #   inference.enable_model_cpu_offload : flow.params.offload_models