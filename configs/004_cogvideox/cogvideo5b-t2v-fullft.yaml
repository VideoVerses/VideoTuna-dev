model:
  base_learning_rate: 6e-6
  target: videotuna.cogvideo_hf.cogvideo_pl.CogVideoXWorkFlow
  params:
    # VAE of CogVideoX
    first_stage_config:
      target: diffusers.AutoencoderKLCogVideoX
      params:
        pretrained_model_name_or_path: checkpoints/cogvideo/CogVideoX-5b
        subfolder: "vae"

    # Text encoder (T5) of CogVideoX
    cond_stage_config:
      target: videotuna.lvdm.modules.encoders.condition.FrozenT5Embedder
      params:
        version: "DeepFloyd/t5-v1_1-xxl"
        device: "cuda"
        max_length: 226
        freeze: True

    # Denosier model
    denoiser_config:
      target: diffusers.CogVideoXTransformer3DModel
      params:
        pretrained_model_name_or_path: checkpoints/cogvideo/CogVideoX-5b
        subfolder: "transformer"

    # Diffusion sampling scheduler
    scheduler_config:
      target: diffusers.CogVideoXDPMScheduler
      params:
        pretrained_model_name_or_path: checkpoints/cogvideo/CogVideoX-5b
        subfolder: scheduler

data:
  target: videotuna.data.lightning_data.DataModuleFromConfig
  params:
    batch_size: 1
    num_workers: 16
    wrap: false
    train:
      target: videotuna.data.datasets.DatasetFromCSV
      params:
        csv_path: ${YOUR_DATA_CSV_PATH}
        height: 480
        width: 720
        video_length: 49
        frame_interval: 1
        train: True
    validation:
      target: videotuna.data.datasets.DatasetFromCSV
      params:
        csv_path: ${YOUR_DATA_CSV_PATH}
        height: 480
        width: 720
        video_length: 49
        frame_interval: 1
        train: False

# training configs
lightning:
  strategy: deepspeed_stage_2
  trainer:
    benchmark: True
    num_nodes: 1
    accumulate_grad_batches: 2
    max_epochs: 2000
    precision: 16
  callbacks:
    image_logger:
      target: videotuna.utils.callbacks.ImageLogger
      params:
        batch_frequency: 50
        max_images: 2
        to_local: True # save videos into local files
        log_images_kwargs:
          unconditional_guidance_scale: 6
    metrics_over_trainsteps_checkpoint:
      target: pytorch_lightning.callbacks.ModelCheckpoint
      params:
        filename: "{epoch:06}-{step:09}"
        save_weights_only: False
        # every_n_epochs: 300
        every_n_train_steps: 10
